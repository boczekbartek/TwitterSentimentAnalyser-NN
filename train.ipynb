{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from process import *\n",
    "from nn.MLP import MLPNetwork\n",
    "from coding import Coding\n",
    "from utils import get_train_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file: tweets_apple.csv\n"
     ]
    }
   ],
   "source": [
    "train_df = get_train_data()\n",
    "train_df['processed_tokens'] = train_df.full_text.apply(tokenize_and_remove_punkt).apply(stem).apply(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder = Coding()\n",
    "_ = train_df.processed_tokens.apply(coder.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode twitts using dictionary and unify lenghts to num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target number of words of after resizing every tweet \n",
    "target_num_words = 20\n",
    "min_occ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 132,\n",
       " '10': 148,\n",
       " '12': 94,\n",
       " '14': 17,\n",
       " '2': 133,\n",
       " '2018': 178,\n",
       " '3': 160,\n",
       " '32gb': 98,\n",
       " '4': 114,\n",
       " '42mm': 83,\n",
       " '5': 115,\n",
       " '58': 5,\n",
       " 'A': 58,\n",
       " 'AI': 28,\n",
       " 'AR': 6,\n",
       " 'I': 189,\n",
       " 'It': 140,\n",
       " 'RT': 199,\n",
       " 'To': 64,\n",
       " 'We': 37,\n",
       " 'a': 192,\n",
       " 'about': 166,\n",
       " 'access': 172,\n",
       " 'account': 31,\n",
       " 'ahead': 141,\n",
       " 'air': 3,\n",
       " 'airpow': 2,\n",
       " 'all': 90,\n",
       " 'allow': 59,\n",
       " 'amazon': 164,\n",
       " 'amp': 41,\n",
       " 'an': 66,\n",
       " 'and': 197,\n",
       " 'android': 88,\n",
       " 'announc': 78,\n",
       " 'anoth': 42,\n",
       " 'app': 136,\n",
       " 'appl': 205,\n",
       " 'applemus': 40,\n",
       " 'around': 16,\n",
       " 'as': 82,\n",
       " 'at': 184,\n",
       " 'automobil': 7,\n",
       " 'band': 45,\n",
       " 'be': 198,\n",
       " 'becaus': 13,\n",
       " 'blackberri': 24,\n",
       " 'breach': 4,\n",
       " 'broad': 70,\n",
       " 'by': 150,\n",
       " 'can': 74,\n",
       " 'car': 27,\n",
       " 'co': 202,\n",
       " 'coffe': 104,\n",
       " 'compani': 10,\n",
       " 'confer': 1,\n",
       " 'core': 60,\n",
       " 'could': 124,\n",
       " 'data': 187,\n",
       " 'day': 163,\n",
       " 'develop': 153,\n",
       " 'devic': 151,\n",
       " 'donut': 12,\n",
       " 'doubt': 35,\n",
       " 'download': 100,\n",
       " 'enough': 123,\n",
       " 'expect': 159,\n",
       " 'facebook': 191,\n",
       " 'fix': 14,\n",
       " 'food': 109,\n",
       " 'for': 190,\n",
       " 'free': 121,\n",
       " 'from': 156,\n",
       " 'full': 63,\n",
       " 'get': 99,\n",
       " 'give': 181,\n",
       " 'go': 120,\n",
       " 'googl': 19,\n",
       " 'great': 54,\n",
       " 'ha': 118,\n",
       " 'happi': 22,\n",
       " 'have': 173,\n",
       " 'hear': 36,\n",
       " 'here': 154,\n",
       " 'how': 84,\n",
       " 'http': 203,\n",
       " 'icloud': 93,\n",
       " 'in': 176,\n",
       " 'inform': 142,\n",
       " 'io': 186,\n",
       " 'ios12': 143,\n",
       " 'ipad': 97,\n",
       " 'iphon': 183,\n",
       " 'iphonese2': 95,\n",
       " 'iphonex': 65,\n",
       " 'it': 174,\n",
       " 'itun': 69,\n",
       " 'iwatch': 46,\n",
       " 'japan': 129,\n",
       " 'keynot': 57,\n",
       " 'know': 52,\n",
       " 'laptop': 30,\n",
       " 'leak': 68,\n",
       " 'let': 8,\n",
       " 'letitbrew1': 103,\n",
       " 'listen': 144,\n",
       " 'look': 67,\n",
       " 'love': 96,\n",
       " 'mac': 165,\n",
       " 'macbook': 170,\n",
       " 'maco': 155,\n",
       " 'make': 43,\n",
       " 'maker': 134,\n",
       " 'masaki': 127,\n",
       " 'may': 101,\n",
       " 'me': 33,\n",
       " 'messag': 87,\n",
       " 'microsoft': 102,\n",
       " 'mondaymotiv': 122,\n",
       " 'more': 135,\n",
       " 'multi': 26,\n",
       " 'music': 188,\n",
       " 'my': 85,\n",
       " 'new': 179,\n",
       " 'news': 49,\n",
       " 'no': 56,\n",
       " 'notjustlak': 111,\n",
       " 'now': 131,\n",
       " 'of': 194,\n",
       " 'off': 126,\n",
       " 'on': 193,\n",
       " 'one': 34,\n",
       " 'oper': 38,\n",
       " 'other': 116,\n",
       " 'our': 77,\n",
       " 'out': 139,\n",
       " 'peopl': 50,\n",
       " 'person': 117,\n",
       " 'photographi': 108,\n",
       " 'privaci': 113,\n",
       " 'pro': 161,\n",
       " 'ram': 80,\n",
       " 're': 0,\n",
       " 'read': 32,\n",
       " 'readi': 11,\n",
       " 'releas': 75,\n",
       " 'repair': 48,\n",
       " 'report': 112,\n",
       " 'rule': 105,\n",
       " 's': 196,\n",
       " 'sale': 72,\n",
       " 'samsung': 185,\n",
       " 'sandwich': 110,\n",
       " 'save': 73,\n",
       " 'say': 23,\n",
       " 'secur': 25,\n",
       " 'see': 130,\n",
       " 'seri': 146,\n",
       " 'share': 145,\n",
       " 'sign': 9,\n",
       " 'smartwatch': 81,\n",
       " 'sport': 44,\n",
       " 'spotlight': 91,\n",
       " 'start': 53,\n",
       " 'support': 55,\n",
       " 'system': 39,\n",
       " 't': 204,\n",
       " 'tea': 106,\n",
       " 'tech': 137,\n",
       " 'technolog': 138,\n",
       " 'that': 147,\n",
       " 'the': 200,\n",
       " 'their': 62,\n",
       " 'there': 71,\n",
       " 'thi': 119,\n",
       " 'think': 125,\n",
       " 'time': 158,\n",
       " 'to': 201,\n",
       " 'today': 177,\n",
       " 'travel': 107,\n",
       " 'tvo': 86,\n",
       " 'unveil': 61,\n",
       " 'up': 29,\n",
       " 'updat': 76,\n",
       " 'use': 89,\n",
       " 'user': 157,\n",
       " 'via': 79,\n",
       " 'video': 47,\n",
       " 'visit': 51,\n",
       " 'want': 20,\n",
       " 'watch': 162,\n",
       " 'watcho': 18,\n",
       " 'we': 152,\n",
       " 'what': 180,\n",
       " 'while': 15,\n",
       " 'who': 21,\n",
       " 'will': 168,\n",
       " 'with': 169,\n",
       " 'world': 149,\n",
       " 'wwdc': 195,\n",
       " 'wwdc18': 182,\n",
       " 'wwdc2018': 175,\n",
       " 'yoda': 128,\n",
       " 'you': 171,\n",
       " 'young': 92,\n",
       " 'your': 167}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coder.compile(min_threshold=min_occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['coded_tokens'] = (\n",
    "    train_df\n",
    "    .processed_tokens\n",
    "    .apply(lambda l: [coder.encode_final(tok) for tok in l])\n",
    "    .apply(partial(pad_or_truncate, target_len=target_num_words, end=True, pad_value=0))\n",
    "    .apply(np.array)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>processed_tokens</th>\n",
       "      <th>coded_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Amittrajit Ghosh And Ashwin Naik Become First ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[amittrajit, ghosh, and, ashwin, naik, becom, ...</td>\n",
       "      <td>[0, 0, 35, 0, 0, 0, 0, 0, 38, 281, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Hailee Steinfeld To Play Comedy Legend Emily D...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[haile, steinfeld, To, play, comedi, legend, e...</td>\n",
       "      <td>[0, 0, 281, 0, 0, 0, 0, 0, 0, 7, 219, 9, 10, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>RT @fabsanchezp: #WWDC2018 #iOS12 #macOS14\\r\\n...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[RT, fabsanchezp, wwdc2018, ios12, macos14, th...</td>\n",
       "      <td>[16, 0, 41, 117, 0, 0, 0, 86, 93, 7, 123, 58, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>RT @AR72014: #wallpaper #homescreen #lockscree...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[RT, ar72014, wallpap, homescreen, lockscreen,...</td>\n",
       "      <td>[16, 0, 0, 0, 0, 91, 7, 41, 123, 21, 58, 4, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>RT @arthr: Will there be a multi-core/32gb ram...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[RT, arthr, will, there, be, a, multi, core, 3...</td>\n",
       "      <td>[16, 0, 211, 588, 58, 179, 589, 199, 202, 204,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>✪ WWDC 2018: How to Watch #Apple's Keynote on ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[wwdc, 2018, how, to, watch, appl, s, keynot, ...</td>\n",
       "      <td>[31, 32, 352, 57, 241, 7, 38, 39, 6, 0, 0, 129...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>#Apple security updates, #iOS and #macOS now s...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[appl, secur, updat, io, and, maco, now, suppo...</td>\n",
       "      <td>[7, 584, 36, 91, 35, 417, 107, 663, 545, 105, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Apple Shares Animoji Karaoke Ad on its Main Yo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[appl, share, animoji, karaok, Ad, on, it, mai...</td>\n",
       "      <td>[7, 75, 0, 0, 0, 6, 87, 0, 0, 0, 900, 70, 31, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>RT @JurassicApps: Dinosaur Assassin: I-Evoluti...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[RT, jurassicapp, dinosaur, assassin, I, evolu...</td>\n",
       "      <td>[16, 0, 0, 0, 234, 0, 83, 488, 489, 107, 57, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>🌹🌹🌹🌹I give these flowers to #Apple 🌹🌹🌹🌹🌹🌹🌹🌹🌹🌹🌹...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[I, give, these, flower, to, appl, http, t, co...</td>\n",
       "      <td>[234, 518, 0, 0, 57, 7, 9, 10, 11, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             full_text  score  \\\n",
       "24   Amittrajit Ghosh And Ashwin Naik Become First ...    3.0   \n",
       "203  Hailee Steinfeld To Play Comedy Legend Emily D...    3.0   \n",
       "143  RT @fabsanchezp: #WWDC2018 #iOS12 #macOS14\\r\\n...    3.0   \n",
       "187  RT @AR72014: #wallpaper #homescreen #lockscree...    3.0   \n",
       "65   RT @arthr: Will there be a multi-core/32gb ram...    4.0   \n",
       "68   ✪ WWDC 2018: How to Watch #Apple's Keynote on ...    3.0   \n",
       "76   #Apple security updates, #iOS and #macOS now s...    3.0   \n",
       "243  Apple Shares Animoji Karaoke Ad on its Main Yo...    3.0   \n",
       "247  RT @JurassicApps: Dinosaur Assassin: I-Evoluti...    3.0   \n",
       "105  🌹🌹🌹🌹I give these flowers to #Apple 🌹🌹🌹🌹🌹🌹🌹🌹🌹🌹🌹...    5.0   \n",
       "\n",
       "                                      processed_tokens  \\\n",
       "24   [amittrajit, ghosh, and, ashwin, naik, becom, ...   \n",
       "203  [haile, steinfeld, To, play, comedi, legend, e...   \n",
       "143  [RT, fabsanchezp, wwdc2018, ios12, macos14, th...   \n",
       "187  [RT, ar72014, wallpap, homescreen, lockscreen,...   \n",
       "65   [RT, arthr, will, there, be, a, multi, core, 3...   \n",
       "68   [wwdc, 2018, how, to, watch, appl, s, keynot, ...   \n",
       "76   [appl, secur, updat, io, and, maco, now, suppo...   \n",
       "243  [appl, share, animoji, karaok, Ad, on, it, mai...   \n",
       "247  [RT, jurassicapp, dinosaur, assassin, I, evolu...   \n",
       "105  [I, give, these, flower, to, appl, http, t, co...   \n",
       "\n",
       "                                          coded_tokens  \n",
       "24   [0, 0, 35, 0, 0, 0, 0, 0, 38, 281, 0, 0, 0, 0,...  \n",
       "203  [0, 0, 281, 0, 0, 0, 0, 0, 0, 7, 219, 9, 10, 1...  \n",
       "143  [16, 0, 41, 117, 0, 0, 0, 86, 93, 7, 123, 58, ...  \n",
       "187  [16, 0, 0, 0, 0, 91, 7, 41, 123, 21, 58, 4, 31...  \n",
       "65   [16, 0, 211, 588, 58, 179, 589, 199, 202, 204,...  \n",
       "68   [31, 32, 352, 57, 241, 7, 38, 39, 6, 0, 0, 129...  \n",
       "76   [7, 584, 36, 91, 35, 417, 107, 663, 545, 105, ...  \n",
       "243  [7, 75, 0, 0, 0, 6, 87, 0, 0, 0, 900, 70, 31, ...  \n",
       "247  [16, 0, 0, 0, 234, 0, 83, 488, 489, 107, 57, 4...  \n",
       "105  [234, 518, 0, 0, 57, 7, 9, 10, 11, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 35, 68,  4,  0, 70, 71,  0,  0,  0, 75, 76, 71,  0,  0,  9, 10,\n",
       "       11,  0, 70])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.coded_tokens.tolist()[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartek/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(final_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coder.len_between(threshold_min=min_occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(coder.len_between(threshold_min=min_occ)+1, 64))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "# input_array = np.random.randint(1000, size=(32, 10))\n",
    "model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-43b4440a3114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# input_array = np.random.randint(1000, size=(32, 10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0moutput_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_array' is not defined"
     ]
    }
   ],
   "source": [
    "output_array = model.predict()\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.coded_tokens.tolist(), \n",
    "                                                    train_df.score.tolist(), test_size=0.3)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(score):\n",
    "    \"\"\" Reduce number of classes \"\"\"\n",
    "    if score < 3.0:\n",
    "        return 0\n",
    "    elif score > 3.0:\n",
    "        return 2\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training parameters\n",
    "* num_features -> this will mean number of neurons in input layer as well as number of coded tokens in each input tweet passed to network\n",
    "* num_classes -> number of classes, number of neurons in output layer of network\n",
    "* num_hidden_neurons -> number of neurons in hidden layer\n",
    "* num_expamles -> number of examples in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 30\n",
    "num_classes = len(train_df.score.unique())\n",
    "num_hidden_neurons = 400\n",
    "num_examples = len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLPNetwork(num_classes=3,\n",
    "                num_examples=len(X_train),\n",
    "                num_features=num_features,\n",
    "                num_hidden_neurons={1 : 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 3., 3., 2., 5., 3., 4., 3., 3., 2., 5., 3., 3., 4., 4., 3., 1.,\n",
       "       3., 4., 1., 3., 3., 4., 5., 3., 5., 2., 2., 2., 3., 3., 3., 1., 3.,\n",
       "       3., 5., 3., 5., 3., 4., 3., 3., 2., 4., 3., 5., 3., 4., 3., 3., 1.,\n",
       "       4., 3., 3., 3., 1., 1., 3., 3., 3., 4., 3., 4., 4., 4., 5., 3., 3.,\n",
       "       3., 4., 4., 3., 3., 5., 5., 3., 2., 3., 3., 3., 3., 4., 4., 3., 5.,\n",
       "       3., 4., 5., 3., 1., 3., 2., 3., 4., 5., 3., 2., 3., 4., 2., 3., 3.,\n",
       "       3., 2., 5., 4., 4., 3., 3., 5., 3., 2., 2., 3., 4., 3., 4., 3., 3.,\n",
       "       5., 3., 4., 3., 1., 5., 3., 4., 4., 1., 3., 4., 2., 4., 3., 4., 3.,\n",
       "       3., 4., 3., 3., 3., 3., 3., 4., 3., 3., 4., 3., 4., 3., 5., 3., 3.,\n",
       "       4., 3., 1., 3., 2., 3., 3., 4., 4., 3., 4., 4., 3., 3., 3., 3., 3.,\n",
       "       3., 4., 3., 1., 5.])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = net.fit(X_train,y_train, batches=10000, print_loss=True)\n",
    "mod = MLPClassifier(hidden_layer_sizes=(1000,1000), )\n",
    "\n",
    "t = mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21142857142857144\n"
     ]
    }
   ],
   "source": [
    "good_sklearn = 0\n",
    "good_my = 0\n",
    "for i in range(len(y_test)):\n",
    "    pred_sklearn = t.predict([X_test[i]])\n",
    "#     pred_my = net.predict(X_train[i])\n",
    "    if pred_sklearn == y_test[i]:\n",
    "#         print(True)\n",
    "        good_sklearn+=1\n",
    "    if pred_my == y_train[i]:\n",
    "        good_my += 1\n",
    "   \n",
    "# print(float(good_my)/len(y_train))        \n",
    "print(float(good_sklearn)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "0.3076923076923077\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "for i in range(len(y_test)):\n",
    "    if net.predict(X_test[i]) == y_test[i]:\n",
    "        print(True)\n",
    "        good+=1\n",
    "    else:\n",
    "        print(False)\n",
    "        \n",
    "print(float(good)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_text           MASAKI YODA -Carrying the future- on #Apple mu...\n",
       "score                                                               4\n",
       "processed_tokens    [masaki, yoda, carri, the, futur, on, appl, mu...\n",
       "coded_tokens        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 8, 13,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from keras.lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartek/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = load_iris().data, load_iris().target\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (4,100) not aligned: 1 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-ad478eae7f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m8.4\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m2.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 105\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TwitterSentimentAnalyser-NN/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,1) and (4,100) not aligned: 1 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "print(mlp.predict(np.array([3.1,  2.5,  8.4,  2.2]).reshape(-1,1)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp.predict_proba([3.1,  2.5,  8.4,  2.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-228-2829967ea018>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-228-2829967ea018>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(mlp.predict_proba([3.1,  2.5,  8.4,  2.2]))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"sum: %f\"%np.sum(mlp.predict_proba([3.1,  2.5,  8.4,  2.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterSentimentAnalyser-NN",
   "language": "python",
   "name": "twittersentimentanalyser-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
